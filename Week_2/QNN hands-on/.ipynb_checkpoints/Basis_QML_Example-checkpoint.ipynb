{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvoRF79u9R7y"
   },
   "outputs": [],
   "source": [
    "#!pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dy1OtSJ8DZp"
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR_tbn77_0Aj"
   },
   "source": [
    "# **THE PROBLEM**\n",
    "\n",
    "This notebook example was adapted (and mostly copied) by the variational classifier tutorial by Pennylane: https://pennylane.ai/qml/demos/tutorial_variational_classifier.\n",
    "\n",
    "The problem we are faced with is to train a variational quantum classifier, where we can train on labelled datasets to see whether we can classify new data points.\n",
    "In this particular example, we want to see whether we can reproduce a parity function, which is basically checking whether the number of ones in a binary vector is even or odd.\n",
    "This is a nice toy example so that we can see how the Basis Encoding works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkR7P4DBEAfq"
   },
   "source": [
    "# **THE DATA**\n",
    "\n",
    "We first recreate the parity function in the form of a dataset.\n",
    "Since we want to use 4 qubits, we note that that means we have 16 possible inputs (and outputs).\n",
    "\n",
    "To get the labels, we denote those with an uneven number of ones to be 1 and those with an even number of ones to be -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr05DckL8Nko"
   },
   "outputs": [],
   "source": [
    "# Possible binary inputs\n",
    "\n",
    "X = list(map(list, itertools.product([0, 1], repeat=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9KLTDr79prA",
    "outputId": "40ded386-5c9e-40e1-ff45-dd6d6989b347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Find labels according to parity function\n",
    "\n",
    "Y = []\n",
    "for i in X:\n",
    "  if (sum(i) % 2 == 0):\n",
    "    Y.append(-1)\n",
    "  else:\n",
    "    Y.append(1)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ecdD6VC9ug_",
    "outputId": "b73a0161-1c42-4968-d6c1-7aad91504605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0 0 0 0], Y = -1\n",
      "X = [0 0 0 1], Y =  1\n",
      "X = [0 0 1 0], Y =  1\n",
      "X = [0 0 1 1], Y = -1\n",
      "X = [0 1 0 0], Y =  1\n",
      "X = [0 1 0 1], Y = -1\n",
      "X = [0 1 1 0], Y = -1\n",
      "X = [0 1 1 1], Y =  1\n",
      "X = [1 0 0 0], Y =  1\n",
      "X = [1 0 0 1], Y = -1\n",
      "X = [1 0 1 0], Y = -1\n",
      "X = [1 0 1 1], Y =  1\n",
      "X = [1 1 0 0], Y = -1\n",
      "X = [1 1 0 1], Y =  1\n",
      "X = [1 1 1 0], Y =  1\n",
      "X = [1 1 1 1], Y = -1\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X, requires_grad=False)\n",
    "Y = np.array(Y, requires_grad=False)\n",
    "\n",
    "for i in range(16):\n",
    "    print(\"X = {}, Y = {: d}\".format(X[i], int(Y[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmLIewPXFEaH"
   },
   "source": [
    "# **THE MODEL**\n",
    "\n",
    "We just use native Pennylane for this. Note that with Pennylane, it is easy to adapt to interface (like Braket or Qiskit) since we are tapping on their quantum computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzVfMS6d-jCk"
   },
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "# Basis Encoding\n",
    "\n",
    "def statepreparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])\n",
    "\n",
    "# Variational Layer\n",
    "\n",
    "def layer(W):\n",
    "\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
    "    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfWfqxA1GGu8"
   },
   "outputs": [],
   "source": [
    "# Variational Circuit\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "\n",
    "    statepreparation(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Model\n",
    "\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkYlBAMu8NsH"
   },
   "outputs": [],
   "source": [
    "# We use square loss as the cost function\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# And we use accuracy to see how well our model does\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FroSdyhXGq09"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Io4wg1EmGqU9",
    "outputId": "31b0d05f-e90a-451e-abce-07f7a5478fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]]\n",
      "\n",
      " [[ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]]] 0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(weights_init, bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgTg3tmLhTyI",
    "outputId": "693bfef4-4dca-42b2-b386-027f7d6a38f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ─╭|Ψ⟩──Rot(0.02,0.00,0.01)───╭●───────╭X──Rot(0.01,0.00,0.00)───╭●───────╭X─┤  <Z>\n",
      "1: ─├|Ψ⟩──Rot(0.02,0.02,-0.01)──╰X─╭●────│───Rot(0.00,0.01,-0.00)──╰X─╭●────│──┤     \n",
      "2: ─├|Ψ⟩──Rot(0.01,-0.00,-0.00)────╰X─╭●─│───Rot(0.00,-0.01,-0.03)────╰X─╭●─│──┤     \n",
      "3: ─╰|Ψ⟩──Rot(0.00,0.00,0.01)─────────╰X─╰●──Rot(0.01,0.01,-0.01)────────╰X─╰●─┤     \n"
     ]
    }
   ],
   "source": [
    "# Visualize our circuit\n",
    "\n",
    "drawer = qml.draw(circuit)\n",
    "print(drawer(weights_init, X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmF1eFENG0HP"
   },
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsV3DsSxG3E-",
    "outputId": "f0f90e50-4961-4fc5-e1f7-0c83230c2804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 3.4355534 | Accuracy: 0.5000000 \n",
      "Iter:     2 | Cost: 1.9287800 | Accuracy: 0.5000000 \n",
      "Iter:     3 | Cost: 2.0341238 | Accuracy: 0.5000000 \n",
      "Iter:     4 | Cost: 1.6372574 | Accuracy: 0.5000000 \n",
      "Iter:     5 | Cost: 1.3025395 | Accuracy: 0.6250000 \n",
      "Iter:     6 | Cost: 1.4555019 | Accuracy: 0.3750000 \n",
      "Iter:     7 | Cost: 1.4492786 | Accuracy: 0.5000000 \n",
      "Iter:     8 | Cost: 0.6510286 | Accuracy: 0.8750000 \n",
      "Iter:     9 | Cost: 0.0566074 | Accuracy: 1.0000000 \n",
      "Iter:    10 | Cost: 0.0053045 | Accuracy: 1.0000000 \n",
      "Iter:    11 | Cost: 0.0809483 | Accuracy: 1.0000000 \n",
      "Iter:    12 | Cost: 0.1115426 | Accuracy: 1.0000000 \n",
      "Iter:    13 | Cost: 0.1460257 | Accuracy: 1.0000000 \n",
      "Iter:    14 | Cost: 0.0877037 | Accuracy: 1.0000000 \n",
      "Iter:    15 | Cost: 0.0361311 | Accuracy: 1.0000000 \n",
      "Iter:    16 | Cost: 0.0040937 | Accuracy: 1.0000000 \n",
      "Iter:    17 | Cost: 0.0004899 | Accuracy: 1.0000000 \n",
      "Iter:    18 | Cost: 0.0005290 | Accuracy: 1.0000000 \n",
      "Iter:    19 | Cost: 0.0024304 | Accuracy: 1.0000000 \n",
      "Iter:    20 | Cost: 0.0062137 | Accuracy: 1.0000000 \n",
      "Iter:    21 | Cost: 0.0088864 | Accuracy: 1.0000000 \n",
      "Iter:    22 | Cost: 0.0201912 | Accuracy: 1.0000000 \n",
      "Iter:    23 | Cost: 0.0060335 | Accuracy: 1.0000000 \n",
      "Iter:    24 | Cost: 0.0036153 | Accuracy: 1.0000000 \n",
      "Iter:    25 | Cost: 0.0012741 | Accuracy: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(25):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X, Y), acc\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
