{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f669b5e0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9d344a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5e78e",
   "metadata": {},
   "source": [
    "# Why LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ebc55e",
   "metadata": {},
   "source": [
    "Some studies have shown that the LSTM's efficiency and trainability can be improved by replacing some of the layers in the LSTM with variational quantum layers, thus making it a quantum-classical hybrid model of LSTM which we will call QLSTM for Quantum LSTM. In the study done by Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang, they show that QLSTM offers better trainability compared to its classical counterpart as it proved to learn significantly more information after the first training epoch than its classical counterpart, learnt the local features better, all while having a comparable number of parameters. Inspired by these recent results, we wanted to test this variational quantum-classical hybrid neural network technique on stock price predictions. \n",
    "\n",
    "This study into QLSTM was motivated by a combination of a few separate studies:\n",
    "\n",
    "Stock price prediction use BERT and GAN: https://arxiv.org/pdf/2107.09055.pdf, Priyank Sonkiya, Vikas Bajpai, Anukriti Bansal <br>\n",
    "Quantum Long Short-Term Memory: https://arxiv.org/pdf/2009.01783.pdf, Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang <br>\n",
    "\n",
    "With code and ideas reused and repurposed from the following sources:\n",
    "\n",
    "Example of a QLSTM: https://github.com/rdisipio/qlstm, Riccardo Di Sipio <br>\n",
    "How to use PyTorch LSTMs for time series regression: https://www.crosstab.io/articles/time-series-pytorch-lstm, Brian Kent <br>\n",
    "Using GANs to predict stock price movement: https://towardsdatascience.com/aifortrading-2edd6fac689d, Boris Banushev<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf8604",
   "metadata": {},
   "source": [
    "# How does LSTM work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15974e88",
   "metadata": {},
   "source": [
    "It has 2 memory states: \n",
    "- Long Term Memory (cell state) \n",
    "- Short Term Memory (hidden state) \n",
    "\n",
    "And it has 3 gates to work through: \n",
    "- Input gate \n",
    "- Forget gate \n",
    "- Output gate\n",
    "\n",
    "For the Forget gate:\n",
    "- Intuition: Decides what information to forget from the cell state (i.e the long term memory) \n",
    "- Consists of a sigmoid activation layer (0, 1) \n",
    "\n",
    "$$ f_{t} = \\sigma(Linear(x_t + h_t))  $$\n",
    "\n",
    "For the Input gate:\n",
    "- Intuition: Update Cell State, which adds new information to the long term memory\n",
    "- Consists of an element-wise product of a sigmoid activation layer (0, 1) and a tanh activation layer (-1, 1). \n",
    "- The sigmoid layer judges the importance of the new information is, while the tanh layer processes the new information \n",
    "\n",
    "$$ i_{t} = \\sigma(Linear(x_t + h_t))  $$\n",
    "$$ g_{t} = tanh(Linear(x_t + h_t))  $$\n",
    "\n",
    "For the Output gate:\n",
    "- Intuition: Determines what information from the cell state would be used for the output\n",
    "- Consists of a sigmoid activation layer (0, 1) \n",
    "\n",
    "$$ o_{t} = \\sigma(Linear(x_t + h_t))  $$\n",
    "\n",
    "To add them all up:\n",
    "\n",
    "We first update the cell state (i.e the long term memory) using the forget gate (which acts on the previous cell state) and the input gate (which decides uses two layers to process the new information):\n",
    "\n",
    "$$ c_{t} = f_{t} \\dot c_{t-1} + i_{t} \\dot g_{t} $$\n",
    "\n",
    "Then, we use the output gate to determine what information from the long term memory we want to use to determine the information used in the short term memory (our output):\n",
    "\n",
    "$$ h_{t} = o_{t} \\dot tanh(c_{t})  $$\n",
    "\n",
    "With this information, please build the LSTM cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becc38f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        #self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        \n",
    "        self.clayer_forget = torch.nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.clayer_input = torch.nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.clayer_update = torch.nn.Linear(self.concat_size, self.hidden_size)\n",
    "        self.clayer_output = torch.nn.Linear(self.concat_size, self.hidden_size)\n",
    "        \n",
    "        #self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            #y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_forget(v_t))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_input(v_t))  # input block\n",
    "            g_t = torch.tanh(self.clayer_update(v_t))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_output(v_t)) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c79fb",
   "metadata": {},
   "source": [
    "We can use it to create a simple LSTM model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8784852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f72897",
   "metadata": {},
   "source": [
    "# How do we add Quantum to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e84d42",
   "metadata": {},
   "source": [
    "We replace the Linear Layers with Variational Quantum Layers! Therefore, the equations now look like:\n",
    "\n",
    "$$ f_{t} = \\sigma(VQC(x_t + h_t))  $$\n",
    "$$ i_{t} = \\sigma(VQC(x_t + h_t))  $$\n",
    "$$ g_{t} = tanh(VQC(x_t + h_t))  $$\n",
    "$$ o_{t} = \\sigma(VQC(x_t + h_t))  $$\n",
    "$$ c_{t} = f_{t} \\dot c_{t-1} + i_{t} \\dot g_{t} $$\n",
    "$$ h_{t} = o_{t} \\dot tanh(c_{t})  $$\n",
    "\n",
    "To do that, we use Pennylane to create a dressed quantum circuit for each VQC layer:\n",
    "\n",
    "Linear_out(Variational Quantum Circuit(Linear_in(Input)))\n",
    "\n",
    "Here is how you do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e95eb3-b992-496c-82b3-237989db940d",
   "metadata": {},
   "source": [
    "First, let's think about our entangling layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242e415b-370c-44ea-a00b-8ec18f260055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_qubits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6f4999-e8b1-4519-b98a-ed09d51c9507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@qml.qnode(qml.device(\"braket.local.qubit\", wires=n_qubits))\n",
    "def ansatz_visual(params):\n",
    "    # Entangling layer.\n",
    "    for j in range(n_qubits):\n",
    "        if j + 1 < n_qubits:\n",
    "            qml.CNOT(wires=[j, j + 1])\n",
    "        else:\n",
    "            qml.CNOT(wires=[j, j + 1 - n_qubits])\n",
    "\n",
    "    # Variational layer.\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[0][i], wires=i)\n",
    "        qml.RY(params[1][i], wires=i)\n",
    "        qml.RZ(params[2][i], wires=i)\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483cbb54-1d81-4745-913e-37cba1b28a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ─╭●───────╭X──RX(1.10)──RY(2.10)──RZ(3.10)─┤  <Z>\n",
      "1: ─╰X─╭●────│───RX(1.20)──RY(2.20)──RZ(3.20)─┤     \n",
      "2: ────╰X─╭●─│───RX(1.30)──RY(2.30)──RZ(3.30)─┤     \n",
      "3: ───────╰X─╰●──RX(1.40)──RY(2.40)──RZ(3.40)─┤     \n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(ansatz_visual)(params=torch.Tensor([[1.1,1.2,1.3,1.4],\n",
    "                                                   [2.1,2.2,2.3,2.4],\n",
    "                                                   [3.1,3.2,3.3,3.4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8046504-6ebc-487c-829c-65ae3b04ad33",
   "metadata": {},
   "source": [
    "Next, let's think about our whole ansatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52924d5a-f547-4a2d-940a-66d61c3f96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qlayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2319ff7f-0e8a-4ae2-a5a1-9c4b4ec7ebab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ansatz_test(params):\n",
    "    # Entangling layer.\n",
    "    for j in range(n_qubits):\n",
    "        if j + 1 < n_qubits:\n",
    "            qml.CNOT(wires=[j, j + 1])\n",
    "        else:\n",
    "            qml.CNOT(wires=[j, j + 1 - n_qubits])\n",
    "\n",
    "    # Variational layer.\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[0][i], wires=i)\n",
    "        qml.RY(params[1][i], wires=i)\n",
    "        qml.RZ(params[2][i], wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c51615e-d970-44da-8503-cf3c4f67af64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@qml.qnode(qml.device(\"braket.local.qubit\", wires=n_qubits))\n",
    "def VQC_visual(features, weights):\n",
    "    # Preprocess input data to encode the initial state.\n",
    "    ry_params = [torch.arctan(feature) for feature in features]\n",
    "    rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "    for i in range(n_qubits):\n",
    "        qml.Hadamard(wires=i)\n",
    "        qml.RY(ry_params[i], wires=i)\n",
    "        qml.RZ(rz_params[i], wires=i)\n",
    "\n",
    "    qml.layer(ansatz_test, n_qlayers, weights)\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322ec60d-d024-492f-87d2-6abf1c1c8957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H──RY(0.79)──RZ(0.79)─╭●───────╭X──RX(1.10)──RY(2.10)──RZ(3.10)─┤  <Z>\n",
      "1: ──H──RY(1.11)──RZ(1.33)─╰X─╭●────│───RX(1.20)──RY(2.20)──RZ(3.20)─┤     \n",
      "2: ──H──RY(1.25)──RZ(1.46)────╰X─╭●─│───RX(1.30)──RY(2.30)──RZ(3.30)─┤     \n",
      "3: ──H──RY(1.33)──RZ(1.51)───────╰X─╰●──RX(1.40)──RY(2.40)──RZ(3.40)─┤     \n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(VQC_visual)(features=torch.Tensor([1,2,3,4]), weights =torch.Tensor([[[1.1,1.2,1.3,1.4], [2.1,2.2,2.3,2.4], [3.1,3.2,3.3,3.4]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10976541-b0e1-4feb-a332-f44fb1a14304",
   "metadata": {},
   "source": [
    "Now let's construct our QLSTM class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9fed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"lightning.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer.\n",
    "            for j in range(self.n_qubits):\n",
    "                if j + 1 < self.n_qubits:\n",
    "                    qml.CNOT(wires=[wires_type[j], wires_type[j + 1]])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[wires_type[j], wires_type[j + 1 - self.n_qubits]])\n",
    "\n",
    "            # Variational layer.\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(rz_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b2ed6",
   "metadata": {},
   "source": [
    "And with this, we can use it to create a simple QLSTM model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1b52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, n_qubits=0, n_qlayers=1):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = QLSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            n_qubits = n_qubits,\n",
    "            n_qlayers= n_qlayers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
